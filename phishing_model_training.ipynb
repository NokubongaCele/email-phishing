{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing Email Detection Model Training\n",
    "\n",
    "This notebook demonstrates the process of training a machine learning model to detect phishing emails. The model will be converted to ONNX format for deployment in a web application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required libraries\n",
    "!pip install numpy pandas scikit-learn matplotlib seaborn nltk skl2onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import email\n",
    "import email.parser\n",
    "from email import policy\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# For ONNX conversion\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import onnxruntime as rt\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLTK Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "For this notebook, we'll use the phishing email dataset from Kaggle. You'll need to download it and upload to your Google Colab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Uncomment the following cell to upload the dataset\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Upload your phishing email dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "# Note: Adjust the filename to match your uploaded file\n",
    "df = pd.read_csv('phishing_email_dataset.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='label', data=df)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Email Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Legitimate', 'Phishing'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions\n",
    "\n",
    "We'll create functions to extract features from email content. These should match the feature extraction in the web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define phishing related keywords\n",
    "PHISHING_KEYWORDS = [\n",
    "    'urgent', 'verify', 'account', 'password', 'update', 'bank', 'security', \n",
    "    'alert', 'suspend', 'login', 'click', 'confirm', 'validate', 'immediately',\n",
    "    'paypal', 'credit', 'debit', 'ssn', 'social security', 'limited time',\n",
    "    'offer', 'prize', 'winner', 'lottery', 'inheritance', 'million', 'dollars',\n",
    "    'fraud', 'secure', 'unauthorised', 'unauthorized', 'access', 'unusual',\n",
    "    'activity', 'breach', 'verify', 'verification', 'restricted', 'terminate',\n",
    "    'expire', 'reset', 'cryptocurrency', 'bitcoin'\n",
    "]\n",
    "\n",
    "# Define suspicious URL patterns\n",
    "SUSPICIOUS_URL_PATTERNS = [\n",
    "    r'https?://(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,}(?:/[^/\\s]*)*',  # URLs\n",
    "    r'(?:https?://)?(?:www\\.)?(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]*[a-zA-Z0-9])?\\.)+[a-zA-Z]{2,}(?:/[^/\\s]*)*'  # URLs without protocol\n",
    "]\n",
    "\n",
    "# Feature names for readability\n",
    "FEATURE_NAMES = [\n",
    "    'contains_urgent_words', \n",
    "    'contains_finance_words',\n",
    "    'contains_security_words',\n",
    "    'num_urls', \n",
    "    'num_urls_mismatched_text',\n",
    "    'has_html', \n",
    "    'has_attachments',\n",
    "    'has_common_phishing_phrases',\n",
    "    'email_length',\n",
    "    'num_misspellings',\n",
    "    'contains_ip_urls',\n",
    "    'has_suspicious_sender',\n",
    "    'request_for_credentials',\n",
    "    'email_contains_javascript',\n",
    "    'link_domain_age_fake'  # Placeholder feature\n",
    "]\n",
    "\n",
    "def extract_features(email_content):\n",
    "    \"\"\"\n",
    "    Extract features from email content for phishing detection\n",
    "    \n",
    "    Args:\n",
    "        email_content (str): Raw email content\n",
    "        \n",
    "    Returns:\n",
    "        list: Numerical features for model input\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to parse as email\n",
    "        email_message = email.parser.Parser().parsestr(email_content)\n",
    "        \n",
    "        # If can't parse headers properly, treat as just the body\n",
    "        if not email_message['From'] and not email_message['To'] and not email_message['Subject']:\n",
    "            email_body = email_content\n",
    "            email_headers = {}\n",
    "        else:\n",
    "            # Extract headers\n",
    "            email_headers = {k: v for k, v in email_message.items()}\n",
    "            \n",
    "            # Extract body\n",
    "            if email_message.is_multipart():\n",
    "                email_body = \"\"\n",
    "                for part in email_message.walk():\n",
    "                    content_type = part.get_content_type()\n",
    "                    if content_type == \"text/plain\" or content_type == \"text/html\":\n",
    "                        try:\n",
    "                            payload = part.get_payload(decode=True).decode('utf-8', errors='ignore')\n",
    "                            email_body += payload\n",
    "                        except:\n",
    "                            pass\n",
    "            else:\n",
    "                try:\n",
    "                    email_body = email_message.get_payload(decode=True).decode('utf-8', errors='ignore')\n",
    "                except:\n",
    "                    email_body = email_message.get_payload()\n",
    "    except:\n",
    "        # If parsing fails, assume the input is just the email body\n",
    "        email_body = email_content\n",
    "        email_headers = {}\n",
    "    \n",
    "    # Feature extraction (as defined in the web application)\n",
    "    lowercase_body = email_body.lower()\n",
    "    \n",
    "    # 1. Check for urgent words\n",
    "    urgent_words = ['urgent', 'immediately', 'alert', 'attention', 'important', 'critical']\n",
    "    contains_urgent_words = any(word in lowercase_body for word in urgent_words)\n",
    "    \n",
    "    # 2. Check for finance-related words\n",
    "    finance_words = ['bank', 'account', 'credit', 'debit', 'payment', 'money', 'transfer', 'financial']\n",
    "    contains_finance_words = any(word in lowercase_body for word in finance_words)\n",
    "    \n",
    "    # 3. Check for security-related words\n",
    "    security_words = ['password', 'login', 'verify', 'secure', 'security', 'update', 'confirm']\n",
    "    contains_security_words = any(word in lowercase_body for word in security_words)\n",
    "    \n",
    "    # 4. Count URLs in the email\n",
    "    urls = []\n",
    "    for pattern in SUSPICIOUS_URL_PATTERNS:\n",
    "        urls.extend(re.findall(pattern, email_body))\n",
    "    num_urls = len(urls)\n",
    "    \n",
    "    # 5. Check for URL text vs. href mismatches\n",
    "    href_pattern = r'<a\\s+(?:[^>]*?\\s+)?href=([\"\\'])(.*?)\\1'\n",
    "    href_urls = re.findall(href_pattern, email_body)\n",
    "    num_urls_mismatched_text = 0\n",
    "    \n",
    "    link_text_pattern = r'<a\\s+(?:[^>]*?\\s+)?href=[\"\\'](.+?)[\"\\'](?:[^>]*?)>(.+?)<\\/a>'\n",
    "    for match in re.finditer(link_text_pattern, email_body, re.IGNORECASE | re.DOTALL):\n",
    "        href = match.group(1)\n",
    "        text = match.group(2)\n",
    "        \n",
    "        # Remove HTML tags from link text\n",
    "        clean_text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        # Check if text looks like a URL but doesn't match href\n",
    "        if (re.search(r'https?://\\S+', clean_text) or \n",
    "            re.search(r'www\\.\\S+', clean_text) or \n",
    "            re.search(r'\\S+\\.(com|org|net|edu|gov|co|io)\\S*', clean_text)):\n",
    "            \n",
    "            # Simple domain comparison\n",
    "            if href not in clean_text and clean_text not in href:\n",
    "                num_urls_mismatched_text += 1\n",
    "    \n",
    "    # 6. Check if email contains HTML\n",
    "    has_html = 1 if re.search(r'<html|<body|<div|<span|<table|<a\\s+href', email_body, re.IGNORECASE) else 0\n",
    "    \n",
    "    # 7. Check for attachments\n",
    "    has_attachments = 0\n",
    "    if hasattr(email_message, 'is_multipart') and email_message.is_multipart():\n",
    "        for part in email_message.walk():\n",
    "            if part.get_content_disposition() == 'attachment':\n",
    "                has_attachments = 1\n",
    "                break\n",
    "    \n",
    "    # 8. Check for common phishing phrases\n",
    "    common_phishing_phrases = [\n",
    "        'verify your account', \n",
    "        'update your information',\n",
    "        'confirm your details', \n",
    "        'unusual activity',\n",
    "        'suspicious activity',\n",
    "        'click here to',\n",
    "        'your account will be suspended',\n",
    "        'won a prize',\n",
    "        'claim your reward',\n",
    "        'access will be disabled'\n",
    "    ]\n",
    "    has_common_phishing_phrases = any(phrase in lowercase_body for phrase in common_phishing_phrases)\n",
    "    \n",
    "    # 9. Email length (normalized)\n",
    "    email_length = min(len(email_body) / 5000, 1.0)  # Normalize to 0-1 range\n",
    "    \n",
    "    # 10. Check for potential misspellings\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,15}\\b', email_body)\n",
    "    misspelling_patterns = [\n",
    "        r'[a-z]{2,}[0-9]+[a-z]*',         # Words with numbers mixed in\n",
    "        r'([a-z])\\1{2,}',                 # Characters repeated more than twice\n",
    "        r'[aeiou]{4,}',                   # Too many consecutive vowels\n",
    "        r'[bcdfghjklmnpqrstvwxyz]{5,}'    # Too many consecutive consonants\n",
    "    ]\n",
    "    num_misspellings = 0\n",
    "    for word in words:\n",
    "        if any(re.search(pattern, word.lower()) for pattern in misspelling_patterns):\n",
    "            num_misspellings += 1\n",
    "    num_misspellings = min(num_misspellings / 10, 1.0)  # Normalize to 0-1 range\n",
    "    \n",
    "    # 11. Check for IP-based URLs\n",
    "    ip_url_pattern = r'https?://\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}'\n",
    "    contains_ip_urls = 1 if re.search(ip_url_pattern, email_body) else 0\n",
    "    \n",
    "    # 12. Check for suspicious sender\n",
    "    has_suspicious_sender = 0\n",
    "    if 'From' in email_headers:\n",
    "        sender = email_headers['From'].lower()\n",
    "        suspicious_patterns = [\n",
    "            r'@.*\\..*\\.[a-z]{2,}',        # Multiple subdomain levels\n",
    "            r'@.*[0-9]{4,}',              # Numbers in domain\n",
    "            r'@(?!gmail|yahoo|hotmail|outlook|aol|icloud|protonmail|mail)',  # Uncommon mail providers\n",
    "            r'@.*\\.(ru|cn|top|xyz|tk|ml|ga|cf)',  # Suspicious TLDs\n",
    "        ]\n",
    "        if any(re.search(pattern, sender) for pattern in suspicious_patterns):\n",
    "            has_suspicious_sender = 1\n",
    "    \n",
    "    # 13. Check for requests for credentials\n",
    "    credential_patterns = [\n",
    "        r'enter.*password',\n",
    "        r'update.*credentials',\n",
    "        r'confirm.*account details',\n",
    "        r'verify.*identity',\n",
    "        r'login.*details',\n",
    "        r'your.*pin',\n",
    "        r'security.*code'\n",
    "    ]\n",
    "    request_for_credentials = any(re.search(pattern, lowercase_body) for pattern in credential_patterns)\n",
    "    \n",
    "    # 14. Email contains JavaScript\n",
    "    email_contains_javascript = 1 if re.search(r'<script|javascript:', email_body, re.IGNORECASE) else 0\n",
    "    \n",
    "    # 15. Link domain age (placeholder feature)\n",
    "    link_domain_age_fake = 1  # Default to suspicious (short age)\n",
    "    \n",
    "    # Combine features into a vector\n",
    "    features = [\n",
    "        int(contains_urgent_words),\n",
    "        int(contains_finance_words),\n",
    "        int(contains_security_words),\n",
    "        min(num_urls / 10, 1.0),  # Normalize number of URLs\n",
    "        min(num_urls_mismatched_text / 5, 1.0),  # Normalize mismatched URLs\n",
    "        has_html,\n",
    "        has_attachments,\n",
    "        int(has_common_phishing_phrases),\n",
    "        email_length,\n",
    "        num_misspellings,\n",
    "        contains_ip_urls,\n",
    "        has_suspicious_sender,\n",
    "        int(request_for_credentials),\n",
    "        email_contains_javascript,\n",
    "        link_domain_age_fake\n",
    "    ]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature extraction class for the pipeline\n",
    "class EmailFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Extract features from each email\n",
    "        features = [extract_features(email) for email in X]\n",
    "        return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract features for each email in the dataset\n",
    "# Note: This process can take some time depending on the size of your dataset\n",
    "print(\"Extracting features from emails...\")\n",
    "X_features = []\n",
    "\n",
    "for email_content in df['email_content']:\n",
    "    features = extract_features(email_content)\n",
    "    X_features.append(features)\n",
    "\n",
    "X = np.array(X_features)\n",
    "y = df['label'].values\n",
    "\n",
    "print(f\"Features extracted. Feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train a Gradient Boosting Classifier\n",
    "print(\"Training Gradient Boosting Classifier...\")\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "y_pred_proba = gb_clf.predict_proba(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Legitimate', 'Phishing'],\n",
    "            yticklabels=['Legitimate', 'Phishing'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Phishing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze feature importance\n",
    "feature_importance = gb_clf.feature_importances_\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(np.array(FEATURE_NAMES)[sorted_idx], feature_importance[sorted_idx])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance in Phishing Detection')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform hyperparameter tuning using GridSearchCV\n",
    "print(\"Performing hyperparameter tuning...\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='f1',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"\\nBest parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate the best model on the test set\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nFinal Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_best):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_best):.4f}\")\n",
    "\n",
    "# Display confusion matrix for the best model\n",
    "cm_best = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Legitimate', 'Phishing'],\n",
    "            yticklabels=['Legitimate', 'Phishing'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Best Model)')\n",
    "plt.show()\n",
    "\n",
    "# Display classification report for the best model\n",
    "print(\"\\nClassification Report (Best Model):\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Legitimate', 'Phishing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Model to ONNX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert the best model to ONNX format\n",
    "print(\"Converting model to ONNX format...\")\n",
    "\n",
    "# Define the input features type\n",
    "initial_type = [('float_input', FloatTensorType([None, X.shape[1]]))]\n",
    "\n",
    "# Convert the model\n",
    "onnx_model = convert_sklearn(best_model, initial_types=initial_type, options={\"zipmap\": True})\n",
    "\n",
    "# Save the ONNX model\n",
    "onnx_model_path = 'phishing_model.onnx'\n",
    "with open(onnx_model_path, 'wb') as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"ONNX model saved to {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the ONNX model with a sample input\n",
    "print(\"Testing ONNX model...\")\n",
    "\n",
    "# Create an ONNX inference session\n",
    "sess_options = rt.SessionOptions()\n",
    "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "onnx_session = rt.InferenceSession(onnx_model_path, sess_options=sess_options)\n",
    "\n",
    "# Get input name\n",
    "input_name = onnx_session.get_inputs()[0].name\n",
    "\n",
    "# Prepare a sample input\n",
    "sample_input = X_test[:5].astype(np.float32)\n",
    "\n",
    "# Run inference with ONNX Runtime\n",
    "onnx_pred = onnx_session.run(None, {input_name: sample_input})\n",
    "\n",
    "# Compare original model predictions with ONNX model predictions\n",
    "original_pred = best_model.predict(X_test[:5])\n",
    "onnx_pred_labels = onnx_pred[0]\n",
    "\n",
    "print(\"\\nComparison of predictions:\")\n",
    "print(\"Original model predictions:\", original_pred)\n",
    "print(\"ONNX model predictions:\", onnx_pred_labels)\n",
    "\n",
    "# Check if the predictions match\n",
    "is_match = np.array_equal(original_pred, onnx_pred_labels)\n",
    "print(f\"Predictions match: {is_match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download the ONNX model file\n",
    "from google.colab import files\n",
    "files.download(onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've:\n",
    "\n",
    "1. Loaded and preprocessed a phishing email dataset\n",
    "2. Extracted meaningful features from emails\n",
    "3. Trained a Gradient Boosting classifier\n",
    "4. Performed hyperparameter tuning to optimize the model\n",
    "5. Evaluated the model's performance\n",
    "6. Converted the model to ONNX format for deployment\n",
    "7. Tested the ONNX model to ensure it works correctly\n",
    "\n",
    "The model is now ready to be integrated into the web application for phishing email detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
